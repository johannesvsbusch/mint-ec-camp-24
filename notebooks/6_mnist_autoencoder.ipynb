{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import image_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Autoencoders\n",
    "Let's try to encode and decode the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load and Visualize the Dataset\n",
    "Let's first load the data and look at some examples of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and separate it into a train and a test set\n",
    "(x_train, _), (x_test, y_test) = image_classification.load_mnist()\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "image_classification.visualize_mnist(images = x_test, labels = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Building A Neural Network To Predict the Input\n",
    "Try to build a neural network that takes `x` as an input and outputs `x` itself! We start with some architecture has a few hidden layers. Can you simplify it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(2000, activation=\"relu\"),\n",
    "        keras.layers.Dense(1000, activation=\"relu\"),\n",
    "        keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "        keras.layers.Reshape([28, 28, 1])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Let's fit the model with the training data!\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics=[\"mse\"])\n",
    "model.fit(x_train, x_train, epochs=10, batch_size=32, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred_nn = model.predict(x_test)\n",
    "image_classification.visualize_mnist(images = x_test, labels = y_test, idxs=[0, 1, 2, 3, 4, 5])\n",
    "image_classification.visualize_mnist(images = x_pred_nn, labels = y_test, idxs=[0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6.2:\n",
    "1. What is the minimal architecture that performs well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Autoencoders\n",
    "Now we create an autoencoder consisting of an **encoder** and a **decoder**. Try different architectures and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our encoder architecture.\n",
    "encoder = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(250, activation=\"relu\"),\n",
    "        keras.layers.Dense(50, activation=\"relu\"),\n",
    "        keras.layers.Dense(2, activation=\"relu\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Our decoder architecture.\n",
    "decoder = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Input(shape=[2]),\n",
    "        keras.layers.Dense(50, activation=\"relu\"),\n",
    "        keras.layers.Dense(250, activation=\"relu\"),\n",
    "        keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "        keras.layers.Reshape([28, 28])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Together, the encoder and decoder form the autoencoder.\n",
    "autoencoder = keras.Sequential([encoder, decoder])\n",
    "\n",
    "# Compile and fit the autoencoder on the train data.\n",
    "autoencoder.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"binary_crossentropy\", \"mse\"])\n",
    "autoencoder.fit(x_train, x_train, epochs=30, batch_size=32, shuffle=True, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print out our performance on the test set.\n",
    "score = autoencoder.evaluate(x_test, x_test, verbose=0)\n",
    "print(f\"Test score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Visualizing the reconstructed images of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = autoencoder.predict(x_test)\n",
    "image_classification.visualize_mnist(images = x_test, labels = y_test, idxs=[0, 1, 2, 3, 4, 5])\n",
    "image_classification.visualize_mnist(images = x_pred, labels = y_test, idxs=[0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.4:\n",
    "1. What is the input and output of the encoder?\n",
    "2. What is the input and output of the decoder?\n",
    "3. Explain how images can be compressed with an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Create your own images\n",
    "The latent space vectors $z = (z_1, z_2, ..., z_m)$ are the output of the encoder network. Each $z_i$ is a value between $-1$ and $1$. Try to construct new images in the following way:\n",
    "1. Create a vector $z = (z_1, z_2, ..., z_m)$ that matches the output dimension of your encoder.\n",
    "2. Use the decoder to estimate $x$.\n",
    "\n",
    "Afterwards, we plot your newly created image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose your own z here.\n",
    "z = np.array([[0.4, 12]])\n",
    "x = decoder.predict(z)\n",
    "plt.imshow(x[0], cmap=\"gray\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Sweeping over the latent space\n",
    "For two-dimensional latent spaces $z = (z_1, z_2)$, we can visualize the entire space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert encoder.layers[-1].output.shape[-1] == 2\n",
    "image_classification.sweep_embedding_space(encoder, decoder, x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
