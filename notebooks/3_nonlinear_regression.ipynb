{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f34e17-c5b0-4682-b327-0df7460c13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nonlinear_regression as nlr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea8f04-afba-4dcb-9500-de5bbc17cefe",
   "metadata": {},
   "source": [
    "# 3. Nonlinear Regression\n",
    "We now take a look at a regression problem where a linear function is not a good model of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb4210-2c12-4001-9fee-4420e3a8c679",
   "metadata": {},
   "source": [
    "## 3.1 Load and Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbc624-8a82-45bf-808c-878c7c1861bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nlr.load_nonlinear_cos_dataset(noise=0.1)\n",
    "nlr.scatter_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70fe1e-7ed2-48bd-8165-231cfe765467",
   "metadata": {},
   "source": [
    "## 3.2 Fitting a Polynomial to the Data\n",
    "The dataset seems to be a nonlinear function. Let's try a polynomial feature transformation again. Try changing the parameter `degree` from 1 to 30 and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9060e5c-bd11-4b2f-8cbc-bce570c944e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=1)\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "pipe = sklearn.pipeline.make_pipeline(poly, lr)\n",
    "pipe.fit(X, y)\n",
    "\n",
    "nlr.plot_prediction(X, y, pipe)\n",
    "\n",
    "mae = sklearn.metrics.mean_absolute_error(y, pipe.predict(X))\n",
    "print(f\"The MAE of the LR model is {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12082a64-50a4-4783-ad76-bf7f5b19ce23",
   "metadata": {},
   "source": [
    "### Q3.2\n",
    "1. For which polynomial order (parameter `degree`) do you measure the smallest Mean Absolute Error (MAE)?\n",
    "2. Which polynomial order  do you think fits the data the best?\n",
    "3. What happens if `degree` is large, e.g., larger than 25?\n",
    "4. What happens if `degree` is small, e.g., one, two, or three?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffffc83e-df18-4063-8df0-5375870fb2d4",
   "metadata": {},
   "source": [
    "## 3.3 Train-Test Split\n",
    "To detect under- and overfitting with metrics, we need to split our entire dataset into two parts: a training set and a test set. The training set is used to train our model. The test set is used to measure performance. Let's repeat the process from above, just that this time we use a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf301dda-5c7b-4a72-80e0-a44ba704e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test set\n",
    "X_train, y_train = nlr.load_nonlinear_pol_dataset(n=30, seed=1)\n",
    "X_test, y_test = nlr.load_nonlinear_pol_dataset(n=100, seed=2)\n",
    "\n",
    "# Plot the two sets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axes[0].set_title(\"Train data\")\n",
    "axes[1].set_title(\"Test data\")\n",
    "nlr.scatter_data(X_train, y_train, axes[0])\n",
    "nlr.scatter_data(X_test, y_test, axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20d987-8543-4c06-a7a6-14f2d2715a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=1)\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "pipe = sklearn.pipeline.make_pipeline(poly, lr)\n",
    "\n",
    "# Train the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "nlr.plot_prediction(X_test, y_test, pipe)\n",
    "\n",
    "# Evaluate the model\n",
    "mae_train = sklearn.metrics.mean_absolute_error(y_train, pipe.predict(X_train))\n",
    "mae_test = sklearn.metrics.mean_absolute_error(y_test, pipe.predict(X_test))\n",
    "print(f\"The MAE of the LR(degree={poly.degree}) model on the train set is {mae_train}\")\n",
    "print(f\"The MAE of the LR(degree={poly.degree}) model on the test set is {mae_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bc112-9508-41b6-9a38-7d8e5f0d6bfc",
   "metadata": {},
   "source": [
    "### Q3.3\n",
    "1. Why should you split your dataset into a train and test set?\n",
    "2. Which degree of polynomials fits the test set best? Is it lower or higher than the one that fits the train set best?\n",
    "3. How can you detect overfitting / underfitting with error metrics?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
